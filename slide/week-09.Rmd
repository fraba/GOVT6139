---
title: "GOVT6139</br>Research design"
subtitle: "Week 09</br>Quantitative Methods"
author: "Francesco Bailo"
institute: "The University of Sydney"
date: "Semester 2, 2025 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling

---

background-image: url(https://upload.wikimedia.org/wikipedia/en/6/6a/Logo_of_the_University_of_Sydney.svg)
background-size: 95%

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      dev = 'svg', out.width = "45%", fig.width = 6,
                      fig.align="center")

options(scipen = 999)
```

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and  recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.


---


## Course outline

.pull-left[

**Week 1**: Introduction

**PART I PRELIMINARY CONSIDERATIONS (Weeks 2-5)**

**Week 2**: The Selection of a Research Approach + Guest lecture w/t Inessa De Angelis

**Week 3**: Review of the Literature

**Week 4**: The Use of Theory + Guest lecture w/t Assel Mussagulova

**Week 5**: Writing Strategies and Ethical Considerations

**PART II DESIGNING RESEARCH (Weeks 6-12)**

**Week 6**: The Introduction


]

.pull-right[


**Week 7**: The Purpose Statement

**Week 8**: Research Questions and Hypotheses + Guest Lecture w/t Minglu Chen

‚è∏Ô∏è *Mid Semester break* ‚è∏Ô∏è

**Week 9**: Quantitative Methods  üëà

**Week 10**: Quantitative Methods: Data Analysis Lab (Make sure you install JASP)

**Week 11**: Qualitative Methods + Guest lecture w/t Sarah Phillips

**Week 12**: Qualitative Methods: Data Analysis Lab (Make sure you install NVivo)

**Week 13**: Conclusions


]


---

## Today's class

| Time         | Content                                    |
|--------------|--------------------------------------------|
| 1:00 - 1:10  | Quantitative approaches to data collection |
| 1:10 - 1:20  | Sampling                                   |
| 1:20 - 1:40  | Task 01 (Group)                            |
| 1:40 - 2:00  | Measurement                                |
| 2:00 - 2:30  | Analysis                                   |
|              | Check-in                                   |
| 2:30 - 3:00  | Task 02 (Individual)                       |

---

## General Feedback on A1

1. *Explain, explain, explain*. 

   - Do not overuse lists! You are not preparing a checklist. When in doubt, drop the list and replace it with a regular paragraph.  
   
   - Reduce direct quotes to the minimum (2-3 word quotes are OK but avoid unjustified longer quotes). When in doubt, drop the quote and explain in your own words (still reference others' ideas appropriately).
   
2. **Unpack** to the lowest possible level (i.e. with as much **explanation** as possible) allowed by the word limit.

   - If you can't unpack something because you don't have enough words available, you are trying to explain too much! Reduce the scope of your explanation.
   
   - RULE 0 is: you can't explain everything. 
   
   - RULE 1 is: you must explain what (e.g., concepts, theories, issues, events) you reference. 
---

3\. **Theory doesn't cause events**! It **explains** them. Make sure that this is reflected in your language.
   - Do not say: "People behave in this way because of that theory." Instead: "That theory is useful to explain why and how people behave in this way". 
   
   - This is not semantics. This is you communicating appropriately your understanding of how theory should be used in your research.  
   
4\. Research design is a **trans-disciplinary language**. It is independent from any discipline but it can talk about research in any discipline. When you write about your research design (e.g., in a research proposal) you are talking from your discipline to people inside and outside your discipline. 
   - You must avoid discipline-specific terminology.
   
   - You must make discipline-specific research problems accessible to people outside that discipline. 
   
   - You must focus your discussion and explanations on your research choices.
---

class: inverse, center, middle

# Chapter 8

# Quantitative Methods

---

# Today's Seminar Learning Objectives

## Quantitative methods

1. The three approaches to collecting data: observation, survey, experiment

2. Sampling: How do you do it?

3. Measurement: Operationalisation and variable construction  

4. Analysis: Bivariate hypothesis tests


---

class: segue

# 1. The three approaches to collecting data: observation, survey, experiment


---

### 1. Observational study (no intervention)

* Quantitative description and association of observed variables (e.g, country-level statistics, social media data, documents)

### 2. Survey design (asking questions, the workhorse of social sciences)

* Quantitative description of trends, attitudes, or opinions of a population
* Testing association
* Studying a sample of that population

### 3. Experimental Design (rarer in the social sciences)

* Systematic manipulation of one or more variables to evaluate an outcome
* Holds other variables constant to isolate effects
* Generalize to a broader population

---

### 1. The three approaches to collecting data: observation, survey, experiment

|                       | Observation | Survey | Experiment |
|-----------------------|-------------|--------|------------|
| Intervention          | NO          | YES    | YES        |
| Variable manipulation | NO          | NO     | YES        |


---
class: segue

# 2. Sampling: How do you do it?

---

### Sampling (obsernational study, survey or experiment)

.center[<img src = "../img/lohr-2021-sampling.png" width = '40%'></img>]

* **Target population** The complete collection of observations we want to study.

* **Sample** A subset of a population.

* **Sampled population** The collection of all possible observation units that might have been chosen in a sample.

* **Sampling unit** A unit that can be selected for a sample. Documents, individuals, households.

* **Sampling frame** A list, map, or other specifiation of sampling units in the population from which a sample may be selected. A list of telephone numbers, street names, farms, documents.


.footnote[Lohr, S. L. (2021). Sampling: Design and Analysis. Chapman and Hall/CRC. https://doi.org/10.1201/9780429298899]


---

.center[<img src = "../img/lohr-2021-sampling.png" width = '55%'></img>]

> **Target population** and **sampled population** in a *telephone survey* of *registered voters*. Some persons in the target population do not have a telephone or will not be associated with a telephone number in the sampling frame. In some households with telephones, the residents are not registered to vote and hence are not eligible for the survey. Some eligible persons in the sampling frame population do not respond because they cannot be contacted, some refuse to respond to the survey, and some may be ill and incapable of responding.

.footnote[Lohr, S. L. (2021). Sampling: Design and Analysis. Chapman and Hall/CRC. https://doi.org/10.1201/9780429298899]


---

Once you have defines your **sampling frame** and your **sample size** ($n$=sample size, $N$=population size), you can finally draw your **sample**. 

How do you do it in practice?


### Types of sampling

- **Random sampling**: You randomly (must be true randomness!) select from your sampling frame (i.e., list)

- **Convenience sampling**: You choose your sample based on convenience and access (more practical, but not ideal!)

---

## Population stratification

Your population is stratified based on characteristics of interest before sampling (e.g., gender, ethnicity). This gives more control on the representation of these characteristics in the final sample. 


.center[<img src = 'https://upload.wikimedia.org/wikipedia/commons/f/fa/Stratified_sampling.PNG' width = '35%'></img>]

.content-box-yellow[
Note: The allocation population-sample can be *proportionate* or *disproportionate*, if you want a larger proportion in your sample of some individuals because their characteristics present more variability.
]


---

class: segue-red

# Task 1: Sampling (group)

.pull-right[

.center[<img src = '../img/padlet-week-09-01.png' width = '55%'></img>]

]

---

class: segue

# 3. Measurement: Operationalisation and variable construction  

---

### A quantitative study proves a theory by 

* testing an hypothesis,

* by measuring through operationalisation abstract concepts. 

.center[<img src = "../img/kellstedt-whitten-2013-theory-hypothesis.png", width = "60%"></img>]

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]
---

### Instrumentation: In your proposal... (from the textbook)
<small>
* Name the survey instrument (e.g. telephone survey) used to collect data
* Indicate how instrument was developed
* Describe the established validity scores (e.g., questions/variables) from past use
  * Content validity
  * Predictive or concurrent validity
  * Construct validity
* Describe reliability of scores from past use
  * Internal consistency
  * Test-retest
* Discuss pilot testing or field-testing
  * Rationale for plans
  * Content validity and reliability
  * Improve question
* Steps for administering for a mailed survey
</small>
---

## Validity vs Reliability (from textbook)

### Validity: can you draw accurate inferences from scores on the instruments?

* **Construct validity**: Does the survey instrument accurately measure the hypothetical construct or concept it was intended to measure?

* **Concurrent or criterion validity**: Does the survey instrument associate with other gold-standard measures of the construct or predict a criterion measure?. 

.content-box-yellow[
Note: **Internal** and **external** validity *mostly* refers to experiment settings. Internal validity is about biases (e.g., selection bias) that can threaten *internal* inferences. External validity is about generalisation of findings outside of the sample, that is *external* inferences about individuals not in the sample.
]

### Reliability: using this instrument (i.e., question) can you consistently repeat the same measurement? 


---

# Probability

---

## What is Probability?
**Probability vs. Statistics**
- **Probability**: Known model ‚Üí predict data
  - "If I flip a fair coin 10 times, what outcomes can I expect?"
- **Statistics**: Observed data ‚Üí infer about model
  - "I saw 10 heads in 10 flips. Is the coin fair?"

*Statistics is learning what we don't know from what we do!*

---

# Two Views of Probability
**1. Frequentist View**
- Probability = long-run frequency
- Flip a coin infinitely many times ‚Üí 50% heads

**2. Bayesian View**
- Probability = degree of belief
- "I'm 60% confident it will rain tomorrow"

*This course focuses on frequentist methods*

---

## Slide 5: Basic Probability Rules
**Key Concepts**
- Probabilities range from 0 (impossible) to 1 (certain)
- All possible outcomes sum to 1
- P(not A) = 1 - P(A)

**Example: Rolling a die**
- P(rolling a 4) = 1/6 ‚âà 0.167
- P(not rolling a 4) = 5/6 ‚âà 0.833

---

## Slide 6: Probability Distributions
**What is a Probability Distribution?**
- Describes how probabilities are spread across possible outcomes
- Two types:
  - **Discrete**: Countable outcomes (e.g., coin flips)
  - **Continuous**: Infinite possible values (e.g., height, IQ)

---

## Slide 7: The Binomial Distribution
**For Counting Successes**
- Used when:
  - Fixed number of trials (N)
  - Each trial has two outcomes (success/failure)
  - Probability of success (Œ∏) is constant
  
**Example**: Rolling 20 dice, counting skulls
- N = 20 trials
- Œ∏ = 1/6 (probability of skull)

---

## Slide 8: Binomial Distribution - Visual
**Example: 20 dice rolls, P(skull) = 1/6**

Most likely outcome: 3-4 skulls
- Getting exactly 10 skulls would be very surprising!
- The distribution shows what to expect

*[Image shows bell-shaped distribution centered around 3-4]*

---

## Slide 9: The Normal Distribution
**The "Bell Curve"**
- Most important distribution in statistics
- Describes many natural phenomena
- Defined by two parameters:
  - **Mean (Œº)**: Center of distribution
  - **Standard deviation (œÉ)**: Spread of distribution

**Examples**: Height, IQ scores, measurement errors

---

## Slide 10: Normal Distribution - Visual
**Properties**
- Symmetric around the mean
- 68.3% of data within 1 SD of mean
- 95.4% of data within 2 SD of mean
- 99.7% of data within 3 SD of mean

*[Shows normal curve with shaded regions]*

**IQ Example**: Œº = 100, œÉ = 15

---

## Slide 11: Why the Normal Distribution Matters
**Three Key Reasons**
1. Many real-world variables are normally distributed
2. Averages of random samples tend toward normal (Central Limit Theorem)
3. Makes statistical inference mathematically tractable

*"The normal distribution is... well, normal!"*

---

## Slide 12: Other Important Distributions
**You'll Encounter These**
- **t-distribution**: Like normal but with heavier tails (for small samples)
- **œá¬≤ (chi-square)**: For categorical data analysis
- **F-distribution**: For comparing variances

*Don't worry about the math - JASP handles these!*

---

## PART 2: ESTIMATING FROM SAMPLES

---

## Slide 13: Samples and Populations
**Key Distinction**
- **Population**: All possible observations we want to understand
  - Often theoretical or impractically large
  - Has true parameters (Œº, œÉ) we don't know
  
- **Sample**: The data we actually collect
  - Finite and observable
  - Has statistics (XÃÑ, s) we can calculate

**Goal**: Use sample statistics to estimate population parameters

---

## Slide 14: Why Sampling Matters
**We Can't Measure Everyone!**
- Too expensive
- Too time-consuming
- Sometimes impossible

**Solution**: Study a sample and make inferences
- Political polls: Sample 1,000 voters from millions
- Drug trials: Sample hundreds from entire population
- Psychology experiments: Sample university students

---

## Slide 15: Simple Random Sampling
**The Gold Standard**
- Every member of population has equal chance of selection
- No bias in selection process
- Allows valid statistical inference

**Reality Check**
- Most psychology samples are "convenience samples"
- Usually undergraduate students
- Must think carefully about generalization

---

## Slide 16: The Law of Large Numbers
**Bigger Samples ‚Üí Better Estimates**

As sample size increases:
- Sample mean gets closer to population mean
- Estimates become more accurate
- Uncertainty decreases

**Mathematical statement**: As N ‚Üí ‚àû, XÃÑ ‚Üí Œº

*"Even the most stupid of men... is convinced that the more observations have been made, the less danger there is of wandering from one's goal" - Jacob Bernoulli, 1713*

---

## Slide 17: Sampling Distributions
**A Crucial Concept**
- If we repeated our experiment many times, we'd get different sample means each time
- The **sampling distribution** shows how these sample means are distributed

**Example**: 
- Draw 5 IQ scores, calculate mean ‚Üí XÃÑ‚ÇÅ = 96
- Draw 5 more IQ scores, calculate mean ‚Üí XÃÑ‚ÇÇ = 107
- Repeat 10,000 times ‚Üí sampling distribution

---

## Slide 18: Sampling Distribution - Visual
**Distribution of Sample Means**
- Center: Population mean (Œº = 100)
- Spread: Standard error (SE)
- Shape: Approximately normal (especially for large N)

*[Shows histogram of 10,000 sample means]*

**Key insight**: Sample means vary, but cluster around truth!

---

## Slide 19: The Central Limit Theorem
**The Most Important Theorem in Statistics**

**Statement**: Regardless of population distribution shape, the sampling distribution of the mean becomes normal as sample size increases

**Implications**:
1. Sample mean is normally distributed (for large N)
2. Mean of sampling distribution = Œº
3. SD of sampling distribution = œÉ/‚àöN (standard error)

---

## Slide 20: Central Limit Theorem - Visual
**Magic of Averaging**

Even if population is skewed:
- N = 2: Sampling distribution less skewed
- N = 4: Nearly normal
- N = 8: Essentially normal

*[Shows progression from skewed to normal distributions]*

**This is why means are so useful!**

---

## Slide 21: Standard Error
**Measuring Uncertainty**

**Standard Error of the Mean (SEM)**:
```
SEM = œÉ/‚àöN
```

**Key points**:
- Smaller SEM = more precise estimate
- Doubling precision requires 4√ó sample size
- This is why large studies are more reliable

**Example**: œÉ = 15, N = 100 ‚Üí SEM = 1.5

---

## Slide 22: Estimating the Population Mean
**Best Guess**
- Population mean: Œº (unknown)
- Sample mean: XÃÑ (calculated from data)
- **Estimate**: ŒºÃÇ = XÃÑ

**Example**:
- Sample 100 people from Port Pirie
- Mean IQ = 98.5
- Best estimate of population mean: ŒºÃÇ = 98.5

*Simple, but not the whole story...*

---

## Slide 23: Estimating Population SD
**A Complication**
- Sample SD: s = ‚àö[Œ£(X·µ¢ - XÃÑ)¬≤/N]
- This is **biased** (systematically too small)

**Solution**: Divide by N-1 instead of N
```
œÉÃÇ = ‚àö[Œ£(X·µ¢ - XÃÑ)¬≤/(N-1)]
```

**Why?** Small samples underestimate variability
- N-1 correction fixes this bias
- JASP uses this formula automatically

---

## Slide 24: Notation Summary
**Keep These Straight!**

| Symbol | Meaning | Known? |
|--------|---------|--------|
| Œº | Population mean | No (almost never) |
| XÃÑ | Sample mean | Yes (calculated) |
| ŒºÃÇ | Estimate of population mean | Yes (= XÃÑ) |
| œÉ | Population SD | No (almost never) |
| s | Sample SD | Yes (biased) |
| œÉÃÇ | Estimate of population SD | Yes (unbiased) |

---

## Slide 25: Confidence Intervals
**Quantifying Uncertainty**

**95% Confidence Interval**:
```
CI‚Çâ‚ÇÖ = XÃÑ ¬± 1.96 √ó (œÉ/‚àöN)
```

**Interpretation** (frequentist):
- If we repeated this experiment many times
- 95% of confidence intervals would contain the true mean
- NOT: "95% probability the true mean is in this interval"

---

## Slide 26: Confidence Intervals - Visual
**Seeing Uncertainty**

*[Shows 50 replications with confidence intervals]*
- Each line represents one experiment
- Most intervals (95%) contain true mean (Œº = 100)
- A few (5%) miss the true mean

**Larger samples ‚Üí narrower intervals ‚Üí more precision**

---

## PART 3: HYPOTHESIS TESTING

---

## Slide 27: The Logic of Hypothesis Testing
**Research Questions Require Hypothesis Tests**

**Example**: Does ESP exist?
- Test 100 people guessing hidden card colors
- 62 get it right (expect 50 if guessing)
- Is this evidence for ESP, or just luck?

**Hypothesis testing provides a formal framework for answering such questions**

---

## Slide 28: Null and Alternative Hypotheses
**Setting Up the Test**

**Null Hypothesis (H‚ÇÄ)**:
- The "boring" hypothesis
- Usually "no effect" or "no difference"
- ESP example: Œ∏ = 0.5 (pure guessing)

**Alternative Hypothesis (H‚ÇÅ)**:
- What we want to demonstrate
- ESP example: Œ∏ ‚â† 0.5 (better than guessing)

**Goal**: Show null hypothesis is (probably) wrong

---

## Slide 29: Hypothesis Testing as a Trial
**Legal Analogy**

- **Defendant**: Null hypothesis
- **Prosecutor**: Researcher
- **Judge**: Statistical test
- **Presumption of innocence**: Assume H‚ÇÄ is true
- **Burden of proof**: Must prove "beyond reasonable doubt"

**We protect the null hypothesis from false conviction!**

---

## Slide 30: Two Types of Errors
**Mistakes We Can Make**

|  | **Retain H‚ÇÄ** | **Reject H‚ÇÄ** |
|---|---|---|
| **H‚ÇÄ true** | ‚úì Correct | ‚úó **Type I error** (Œ±) |
| **H‚ÇÄ false** | ‚úó **Type II error** (Œ≤) | ‚úì Correct (Power) |

**Type I error (Œ±)**: False positive (convicting the innocent)
**Type II error (Œ≤)**: False negative (letting guilty go free)

**We control Type I error rate, typically Œ± = 0.05**

---

## Slide 31: The p-value
**The Most Misunderstood Concept**

**What it is**:
- Probability of observing data this extreme (or more extreme) if H‚ÇÄ is true
- Small p-value = data are surprising under H‚ÇÄ

**What it is NOT**:
- ‚úó Probability that H‚ÇÄ is true
- ‚úó Probability that results are due to chance
- ‚úó Importance of the effect

**Decision rule**: If p < 0.05, reject H‚ÇÄ

---

## Slide 32: Significance Levels
**Conventional Standards**

| **p-value** | **Interpretation** | **Stars** |
|---|---|---|
| p > .05 | Not significant | - |
| p ‚â§ .05 | Significant | * |
| p ‚â§ .01 | Highly significant | ** |
| p ‚â§ .001 | Very highly significant | *** |

**Œ± = 0.05 is arbitrary but conventional**
- 5% chance of Type I error
- "Reasonable doubt" threshold

---

## Slide 33: One-Tailed vs. Two-Tailed Tests
**Direction Matters**

**Two-tailed test**: 
- H‚ÇÅ: Œ∏ ‚â† 0.5 (different in either direction)
- Use when direction unknown
- Most common in psychology

**One-tailed test**:
- H‚ÇÅ: Œ∏ > 0.5 (specific direction)
- Use only when direction predicted in advance
- More powerful but riskier

---

## Slide 34: Effect Size and Power
**Significance ‚â† Importance**

**Effect Size**:
- How big is the difference?
- Cohen's d = (difference in means) / SD
- Rules of thumb: 0.2 (small), 0.5 (medium), 0.8 (large)

**Statistical Power**:
- Probability of detecting a true effect
- Power = 1 - Œ≤
- Increase power: larger sample, larger effect

---

## PART 4: COMPARING TWO MEANS

---

## Slide 35: t-tests
**The Workhorse of Psychology**

**When to use**:
- Outcome variable is continuous (interval/ratio scale)
- Want to compare means between groups
- Most common statistical test in psychology

**Three types**:
1. One-sample t-test
2. Independent samples t-test
3. Paired samples t-test

---

## Slide 36: One-Sample t-test
**Comparing Sample to Known Value**

**Question**: Is our sample mean different from a hypothesized value?

**Example**: 
- Dr. Zeppo's class should average 67.5%
- 20 psychology students average 72.3%
- Is this significantly different?

**Test statistic**:
```
t = (XÃÑ - Œº‚ÇÄ)/(œÉÃÇ/‚àöN)
```

**Result**: t(19) = 2.25, p = .036 ‚Üí Significant!

---

## Slide 37: Independent Samples t-test
**Comparing Two Different Groups**

**Question**: Do two groups have different means?

**Example**:
- Anastasia's students: M = 74.5, SD = 9.0, N = 15
- Bernadette's students: M = 69.1, SD = 5.8, N = 18
- Are grades significantly different?

**Two versions**:
- **Student's t-test**: Assumes equal variances
- **Welch's t-test**: Doesn't assume equal variances (safer)

---

## Slide 38: Paired Samples t-test
**Comparing Two Related Measurements**

**When to use**:
- Same participants measured twice
- Before/after designs
- Matched pairs

**Example**: Dr. Chico's class
- Test 1: M = 57%
- Test 2: M = 58%
- Did students improve significantly?

**Key**: Test the *difference scores* for each person

---

## Slide 39: JASP Hands-On - One Sample t-test
**Step-by-Step**

1. Open JASP ‚Üí Load data
2. T-Tests ‚Üí One Sample T-Test
3. Move variable to "Variables" box
4. Enter test value (e.g., 67.5)
5. Check options:
   - Location parameter
   - Confidence interval (95%)
   - Descriptive plots

**Interpreting Output**:
- t statistic, df, p-value
- Mean difference and CI
- Cohen's d (effect size)

---

## Slide 40: JASP Hands-On - Independent Samples t-test
**Comparing Two Groups**

1. T-Tests ‚Üí Independent Samples T-Test
2. Move outcome variable to "Variables"
3. Move grouping variable to "Grouping Variable"
4. Check options:
   - Student (assumes equal variances)
   - Welch (doesn't assume equal variances)
   - Mann-Whitney (non-parametric alternative)
   - Descriptive plots
   - Assumption checks (normality, equality of variances)

**Read output**:
- Check assumption violations first
- t statistic, df, p-value, CI
- Effect size (Cohen's d)
- Descriptive statistics table

---

## Summary Slide
**Key Takeaways**

1. **Probability** provides foundation for statistical inference
2. **Sampling distributions** describe variability in estimates
3. **Hypothesis tests** formalize scientific decision-making
4. **t-tests** compare means between groups
5. **JASP** makes complex calculations accessible

**Remember**:
- Larger samples ‚Üí more precision
- p < 0.05 ‚Üí reject null hypothesis
- Report effect sizes, not just p-values
- Check assumptions before interpreting results


---

class: segue

# 4. Probability

---

## Why Probability Matters

Imagine a poll says 23% of Australian voters support a party.

**The poll surveyed:** 1,000 voters (*sample* size, $n$)

**Total Australian voters:** 18.126 million (*population* size, $N$)

  - Note that this is not the whole Australian population (~27.2 million) but Australians on "the electoral roll" according to the Australian Electoral Commission (AEC)

**Question:** How can we trust this represents everyone?

**Answer:** Probability theory!

This is the bridge from *describing data* to *making inferences*.

- Describing the data (i.e. our sample!) is what we call **descriptive statistics**

- Using the sample to make observation about the population is what we call **inferential statistics**

---

## Two Views of What we Should Mean with "Probability"

### Frequentist View

- .content-box-yellow[Probability = long-run frequency]
- "If I flip this coin infinitely many times, 50% will be heads"
- **Objective**, but limited to repeatable events

### Bayesian View

- .content-box-yellow[Probability = degree of belief]
- "I'm 80% confident it will rain tomorrow"
- More flexible, but **subjective**

We'll use the frequentist approach (most common in the social sciences).

---

## Probability: The Basics

**Key rules:**
- Probabilities range from 0 (impossible) to 1 (certain)
- All possible outcomes must sum to 1
- P(not A) = 1 - P(A)

**Example:** My trousers
- Blue jeans: P = 0.5
- Grey jeans: P = 0.3
- Black jeans: P = 0.1
- Tracksuit: P = 0.1
- Total = 1.0 ‚úì

---

## The Binomial Distribution

**Scenario:** Roll 20 dice, each has 1/6 chance of showing a skull.

**Question:** What's the probability of getting exactly 4 skulls?

**This is a binomial situation:**
- Fixed number of trials (N = 20)
- Two possible outcomes (skull or not)
- Same probability each time (Œ∏ = 1/6)
- Trials are independent

---

## Binomial Distribution: Visualised
When N=20, Œ∏=1/6 (rolling dice):

Most likely: ~3 skulls
P(exactly 4 skulls) ‚âà 0.20 (20% chance)


**Key insight:** Even with randomness, we can predict patterns!

---

## The Normal Distribution

**The most important distribution in statistics**

Also called:
- The bell curve
- Gaussian distribution

**Why it matters:**
- Appears everywhere in nature
- Foundation for most statistical tests
- Described by just two numbers: mean (Œº) and standard deviation (œÉ)

---

## The Normal Distribution: Shape

**Key properties:**
- Symmetrical around the mean
- 68% of data within ¬±1 SD
- 95% of data within ¬±2 SD
- 99.7% of data within ¬±3 SD

**Example:** IQ scores
- Mean = 100
- SD = 15
- 95% of people score between 70 and 130

---

## Understanding "Area Under the Curve"

**For continuous variables:**
- We can't ask "What's P(temperature = exactly 23¬∞)?" ‚Üí Answer is essentially zero
- Instead: "What's P(temperature between 22.5 and 23.5¬∞)?" ‚Üí This makes sense!

**The area under the curve = probability**

This is why the y-axis says "probability density" not "probability"

---

## Other Important Distributions

You'll meet these later:

**t-distribution:** Like a normal, but with heavier tails (used when we don't know the population SD)

**œá¬≤ distribution:** Skewed, always positive (used for categorical data analysis)

**F-distribution:** For comparing variances (used in ANOVA)

**All are related to the normal distribution!**

---

# Part 2: Sampling and Estimation

---

## The Fundamental Problem

**We want to know:** What's the average IQ in Port Pirie?

**We have:** IQ scores from 100 people (sample)

**Population:** All ~14,000 residents

**Challenge:** How do we learn about the whole population from our sample?

---

## Samples vs Populations

**Population**
- The complete group we want to understand
- Has true parameters (Œº, œÉ)
- Usually impossible to measure completely

**Sample**
- Subset we actually observe
- Has statistics we can calculate (xÃÑ, s)
- Our window into the population

**Goal:** Use sample statistics to estimate population parameters

---

## What Makes a "Good" Sample?

**Simple Random Sample:**
- Every member has equal chance of selection
- No bias in selection
- Sampling without replacement (usually)

**Reality check:**
- True random samples are rare in psychology
- We often use convenience samples (e.g., undergrads)
- This is OK if we're careful about what we can claim

---

## Most Samples Aren't Random

**Other sampling methods:**

**Stratified sampling:** Sample separately from subgroups (e.g., by age)

**Snowball sampling:** Participants recruit others (useful for hard-to-reach groups)

**Convenience sampling:** Whoever is available (most psychology studies!)

**Key question:** Does the sampling method bias our results in ways that matter?

---

## The Law of Large Numbers

**Big idea:** Larger samples give better estimates

**Simulation:** Sample mean from samples of different sizes
- N=2: Sample means vary wildly
- N=100: Sample means close to true mean
- N=10,000: Sample means very close to true mean

**As N ‚Üí ‚àû, sample mean ‚Üí population mean**

This justifies our belief that more data = better answers!

---

## Sampling Distributions

**Thought experiment:** Run the same experiment 10,000 times

Each time:
- Sample 5 people
- Calculate mean IQ

**Result:** A distribution of sample means!

This is called the **sampling distribution of the mean**

Even though individual IQs vary widely (SD=15), the sample means cluster much more tightly around the true mean.

---

## The Central Limit Theorem

**The most important theorem in statistics**

**Says:** Even if your data aren't normal, the sampling distribution of the mean *will be* normal (if N is large enough)

**Three key facts:**
1. Mean of sampling distribution = population mean (Œº)
2. SD of sampling distribution = œÉ/‚àöN (called Standard Error)
3. Shape becomes normal as N increases

**This is why the normal distribution is so important!**

---

## Standard Error vs Standard Deviation

**Standard Deviation (SD or s):**
- Measures variability in your *data*
- "How spread out are individual observations?"
- Doesn't change much with sample size

**Standard Error (SE or SEM):**
- Measures uncertainty in your *estimate*
- "How precise is our sample mean?"
- Gets smaller as N increases: SE = SD/‚àöN

**Key insight:** Bigger samples = more precision!

---

## Estimating Population Parameters

**For the mean:**
- Best estimate = sample mean (xÃÑ)
- This is an *unbiased estimator*

**For the standard deviation:**
- We divide by (N-1) instead of N
- This gives us œÉÃÇ (estimate of œÉ)
- Why? The sample SD is systematically too small
- Dividing by (N-1) corrects this bias

**In JASP:** The SD you see uses (N-1) automatically

---

## Confidence Intervals

**The problem:** Our estimate is uncertain

**The solution:** Report a range, not just a point

**95% Confidence Interval:**
- Range of plausible values for the true parameter
- Calculated as: xÃÑ ¬± (1.96 √ó SEM)
- "We're 95% confident the true mean is in this range"

**Example:** Sample mean = 100, SEM = 2
- 95% CI: 100 ¬± (1.96 √ó 2) = [96.08, 103.92]

---

## What Does "95% Confident" Really Mean?

**Common (wrong) interpretation:**
"There's a 95% chance the true mean is in this interval"

**Correct (frequentist) interpretation:**
"If we repeated this study 100 times, about 95 of the intervals would contain the true mean"

**Why the difference?**
- The true mean is fixed (not random)
- Our interval is random (changes with each sample)
- We make probability statements about repeatable events

---

## Visualising Confidence Intervals

**Imagine 50 replications of the same experiment:**
- Each produces a different CI
- About 95% of them contain the true mean
- About 5% miss it (by chance)

**Wider intervals = more uncertainty**
- Smaller N ‚Üí wider CI
- Larger N ‚Üí narrower CI

---

## Calculating CIs in JASP

**Currently:** JASP Descriptives doesn't show CIs directly

**But you can calculate manually:**

1. Get Mean and SE from Descriptives
2. Lower 95% CI = Mean - (1.96 √ó SE)
3. Upper 95% CI = Mean + (1.96 √ó SE)

**Later in the course:** Some analyses will plot CIs automatically

---

## Key Takeaways: Probability

1. Probability lets us quantify uncertainty
2. The normal distribution is everywhere
3. Understanding distributions helps us understand statistical tests
4. Area under the curve = probability for continuous variables

---

## Key Takeaways: Sampling

1. Samples let us learn about populations
2. Larger samples give more precise estimates
3. The Central Limit Theorem explains why means are normally distributed
4. Confidence intervals quantify our uncertainty
5. SE gets smaller as N increases: SE = SD/‚àöN

---

## Next Week: Putting It All Together

**We'll use these concepts to:**
- Test hypotheses about our data
- Make decisions under uncertainty
- Understand when differences are "real" vs chance

**Get ready for:** Hypothesis testing and the t-test!

---

## Questions?

**Remember:**
- Statistics is a tool for dealing with uncertainty
- Perfect knowledge is impossible
- But probability theory helps us make good inferences from imperfect data

**This is what makes science possible!**

---

class: segue

# 4. Analysis: Bivariate hypothesis tests

---

## Variable type and appropriate bivariate hypothesis tests 


> ‚ÄúAre X and Y related?‚Äù By definition ‚Äì ‚Äúbivariate‚Äù means ‚Äútwo variables‚Äù ‚Äì these tests cannot help us with the important question, ‚ÄúHave we controlled for all confounding variables Z that might make the observed association between X and Y spurious?‚Äù 

(We will deal with Z next week!)

<table>
<thead>
  <tr>
    <th></th>
    <th></th>
    <th colspan="2">Independent variable type</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td></td>
    <td></td>
    <td>Categorical</td>
    <td>Continuous</td>
  </tr>
  <tr>
    <td rowspan="2">Dependent variable type</td>
    <td>Catgorical</td>
    <td>Tabular analysis</td>
    <td>Probit/Logit</td>
  </tr>
  <tr>
    <td>Continuous</td>
    <td>Difference of means</td>
    <td>Correlation coefficient, <br>regression model</td>
  </tr>
</tbody>
</table>

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]

---

## The Logic of $p$-Values (and statistical tests)

### Is there covariation between X and Y?

We can answer this question *statistically*, by

* **comparing** the actual (observed) **relationship** between X and Y in **sample data** with

* what **we would expect** to find **if** X and Y **were not related** in the **underlying population**.

Statistical tests (and $p$-values) are about this comparison.

> The p-value, which ranges between 0 and 1, is the probability that we would see the relationship that we are finding because of random chance. Put another way, the p-value tells us the probability that we would see the observed relationship between the two variables in our sample data if there were truly no relationship between them in the unobserved population.

> The **lower** the $p$-value, the **greater confidence** we have that there is a **systematic relationship** between the two variables for which we estimated the particular $p$-value.

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]

---

## From $p$-Values to Statistical Significance

> Lower $p$-values increase our confidence that there is a relationship between the two variables in question. 

> A common way of referring to such a situation is to state that **the relationship between the two variables is statistically significant**.

<br/>

.content-box-red[

When is a relationship statistically significant? We have a number of arbitrary thresholds. 

* In the social sciences, we usually settle to a 0.05 value. That is,

* $p$-value $>0.05$ no statistical significance <large>üòî</large>.

* $p$-value $\le 0.05$ statistical significance and publication <large>üòÄ</large>.

]

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]

---

## Beware of $p$-hacking

The arbitrary threshold at 0.05 creates some perverse incentive in variable selection and reporting so to make sure that your $p$-value will just below it... 

> The distribution of p-values [from 50,000 different tests] exhibits a camel shape with abundant p-values above .25, a valley between .25 and .10 and a bump slightly under .05.


.center[<img src = 'https://i.stack.imgur.com/aYg3y.png' width = '60%'></img>]

.footnote[Brodeur, Abel and Brodeur, Abel and L√â, Mathias and Sangnier, Marc and Zylberberg, Yanos, Star Wars: The Empirics Strike Back (June 18, 2012). Paris School of Economics Working Paper No. 2012-29, Available at SSRN: https://ssrn.com/abstract=2089580 or http://dx.doi.org/10.2139/ssrn.2089580]

---

## Tabular analysis (two categorical variables)

.center[<img src = '../img/kellstedt-whitten-2013-tab-analysis.png'></img>]

How do we test an hypothesis of association between gender and vote? 

Enter the chi-squared ( $\chi^2$ ) test for tabular association

$$ \chi^2 = \sum\frac{(O - E)^2}{E}$$


.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]
---

## Tabular analysis: Prepare the data with R

```{r}
# Create a matrix for the counts of each category
movie_data <- matrix(c(20, 50, 50, 30), nrow = 2, byrow = TRUE,
                     dimnames = list(Preference = c("Action", "Comedy"),
                                     Gender = c("Male", "Female")))

# Display the matrix
print(movie_data)
```

---

## Tabular analysis: Test for statistical association with R

```{r}
# Perform the chi-square test
chi_square_result <- chisq.test(movie_data)

# Print the results
print(chi_square_result)
```


---

## Difference of means (one categorical and one continuous variable)

> In this type of bivariate hypothesis test, we are looking to see if the means are different across the values of the independent variable.

Karl Pearson's $t$-test: 

$$ t = \frac{\bar{Y}_1-\bar{Y}_2}{se(\bar{Y}_1-\bar{Y}_2)} $$ where $se$ is the standard error.


.content-box-green[
Let's test this hypothesis: In the UK Labour-led cabinets last longer than Conservative-led cabinets.]
* $X$ = $party$ $\in \{Con, Lab\}$
* $Y$ = $duration$ $\in \{1,2,3,...\}$

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]

---

## Difference of means: Import data with R

```{r echo = FALSE, eval = FALSE}
library(tidyverse)

dat <- 
  read_csv("https://www.parlgov.org/data/parlgov-development_csv-utf-8/view_cabinet.csv") %>%
  dplyr::filter(country_name == "United Kingdom" &
                  party_name_short %in% c("Con", "Lab"))

dat <-
  dat %>%
  dplyr::filter(prime_minister == 1) %>%
  mutate(duration_days = as.numeric(lead(start_date) - start_date))

write.csv(dat, file = "../data/uk_cabinets.csv", row.names = F)

```


```{r}
my_data <- 
  read.csv("https://raw.githubusercontent.com/fraba/GOVT6139/main/data/uk_cabinets.csv")
```


```{r echo = FALSE}
library(DT)
datatable(my_data, options = list(scrollX = TRUE, pageLength = 4))
```


---

## Difference of means: Visualise data with R (boxplot)

```{r fig.height=3, fig.width=4}
library(ggplot2)
ggplot(data = my_data, aes(x = party_name_short, y = duration_days)) + geom_boxplot()
```

---

## Difference of means: Visualise data with R (density) 

```{r fig.height=3, fig.width=7, out.width='80%'}
library(ggplot2)
ggplot(data = my_data, aes(x = duration_days, colour = party_name_short)) + geom_density()
```

---

## Difference of means: Visualise data with R (t-test) 

After visually inspecting the distribution of the data, let's run the t-test...

```{r}
t_test_result <- t.test(formula =  duration_days ~ party_name_short, data = my_data)
print(t_test_result)
```

**Hypothesis**: In the UK Labour-led cabinets last longer than Conservative-led cabinets.

**Result from statistical analysis**: Although on average Labour-led cabinets last longer (`r round(t_test_result$estimate, 2)`), we can't reject the null hypothesis of no statistical association (as the $p-value$ is `r round(t_test_result$p.value, 2)` $>0.05$).

---

## Correlation coefficient (two continuous variables)

Pearson‚Äôs correlation coefficient

$$ r = \frac{COV_{XY}}{\sqrt{VAR_XVAR_Y}} $$ 

.content-box-yellow[

Let's now test this hypothesis: The larger the proportion of seats in Parliament of the Prime Minister's party, the longer the duration of the cabinet.

]


.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]

--

```{r echo = FALSE}
library(DT)
datatable(my_data, options = list(scrollX = TRUE, pageLength = 2))
```

---

## Correlation coefficient (Code new variable and visualise)

```{r}
my_data$prop_seats <- my_data$seats / my_data$election_seats_total
```

(Note in R, I use `$` to access columns in a dataframe)


```{r fig.height = 3, fig.width=3, out.width="32%"}
ggplot(my_data, aes(x = prop_seats, y = duration_days)) + geom_point()
```

---

## Correlation coefficient (Pearson's test)

```{r}
cor_test_result <- cor.test(x = my_data$prop_seats, y = my_data$duration_days)
print(cor_test_result)
```


Statistical results:

* The two variables are **positively correlated** (cor = `r round(cor_test_result$estimate, 2)`); yet

* The two variables are not **significantly correlated** ( $p$-value = `r round(cor_test_result$p.value, 2)`).

We can't reject the null hypothesis of no association. 

---

# Summary 

1. The three approaches to collecting data: observation, survey, experiment.

2. Sampling: Targe population, Sample, Sampling frame.

3. Measurement: Operationalisation and variable construction.

4. Analysis: Using three bivariate hypothesis tests.

# Reminder

.content-box-red[

* An anonymous evaluation survey is available this week if you want to provide any (always appreciated) feedback on the unit. This is the last survey before the official USS. 

* Next week, remember to install R and RStudio on your laptop (both free, like in free beer) and bring your laptop to class for the data analysis lab!

]

---

class: segue-clue

# Check-in

---

class: segue-red

# Task 2: Hypothesis and questions (individual)

.pull-right[

.center[<img src = '../img/padlet-week-09-02.png' width = '55%'></img>]

]

---