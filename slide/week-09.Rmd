---
title: "GOVT6139</br>Research design"
subtitle: "Week 09</br>Quantitative Methods"
author: "Francesco Bailo"
institute: "The University of Sydney"
date: "Semester 2, 2025 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: ["default", "assets/sydney-fonts.css", "assets/sydney.css"]
    self_contained: false # if true, fonts will be stored locally
    seal: true # show a title slide with YAML information
    includes:
      in_header: "assets/mathjax-equation-numbers.html"
    nature:
      beforeInit: ["assets/remark-zoom.js", "https://platform.twitter.com/widgets.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9' # alternatives '16:9' or '4:3' or others e.g. 13:9
      navigation:
        scroll: false # disable slide transitions by scrolling

---

background-image: url(https://upload.wikimedia.org/wikipedia/en/6/6a/Logo_of_the_University_of_Sydney.svg)
background-size: 95%

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, 
                      dev = 'svg', out.width = "45%", fig.width = 6,
                      fig.align="center")

options(scipen = 999)
```

---

## Acknowledgement of Country

I would like to acknowledge the Traditional Owners of Australia and  recognise their continuing connection to land, water and culture. The  University of Sydney is located on the land of the Gadigal people  of the Eora Nation. I pay my respects to their Elders, past and present.


---


## Course outline

.pull-left[

**Week 1**: Introduction

**PART I PRELIMINARY CONSIDERATIONS (Weeks 2-5)**

**Week 2**: The Selection of a Research Approach + Guest lecture w/t Inessa De Angelis

**Week 3**: Review of the Literature

**Week 4**: The Use of Theory + Guest lecture w/t Assel Mussagulova

**Week 5**: Writing Strategies and Ethical Considerations

**PART II DESIGNING RESEARCH (Weeks 6-12)**

**Week 6**: The Introduction


]

.pull-right[


**Week 7**: The Purpose Statement

**Week 8**: Research Questions and Hypotheses + Guest Lecture w/t Minglu Chen

‚è∏Ô∏è *Mid Semester break* ‚è∏Ô∏è

**Week 9**: Quantitative Methods  üëà

**Week 10**: Quantitative Methods: Data Analysis Lab (Make sure you install JASP)

**Week 11**: Qualitative Methods + Guest lecture w/t Sarah Phillips

**Week 12**: Qualitative Methods: Data Analysis Lab (Make sure you install NVivo)

**Week 13**: Conclusions


]


---

## Today's class

| Time         | Content                                    |
|--------------|--------------------------------------------|
| 1:00 - 1:10  | Quantitative approaches to data collection |
| 1:10 - 1:20  | Sampling                                   |
| 1:20 - 1:40  | Task 01 (Group)                            |
| 1:40 - 2:00  | Measurement                                |
| 2:00 - 2:30  | Analysis                                   |
|              | Check-in                                   |
| 2:30 - 3:00  | Task 02 (Individual)                       |

---

## General Feedback on A1

1. *Explain, explain, explain*. 

   - Do not overuse lists! You are not preparing a checklist. When in doubt, drop the list and replace it with a regular paragraph.  
   
   - Reduce direct quotes to the minimum (2-3 word quotes are OK but avoid unjustified longer quotes). When in doubt, drop the quote and explain in your own words (still reference others' ideas appropriately).
   
2. **Unpack** to the lowest possible level (i.e. with as much **explanation** as possible) allowed by the word limit.

   - If you can't unpack something because you don't have enough words available, you are trying to explain too much! Reduce the scope of your explanation.
   
   - RULE 0 is: you can't explain everything. 
   
   - RULE 1 is: you must explain what (e.g., concepts, theories, issues, events) you reference. 
---

3\. **Theory doesn't cause events**! It **explains** them. Make sure that this is reflected in your language.
   - Do not say: "People behave in this way because of that theory." Instead: "That theory is useful to explain why and how people behave in this way". 
   
   - This is not semantics. This is you communicating appropriately your understanding of how theory should be used in your research.  
   
4\. Research design is a **trans-disciplinary language**. It is independent from any discipline but it can talk about research in any discipline. When you write about your research design (e.g., in a research proposal) you are talking from your discipline to people inside and outside your discipline. 
   - You must avoid discipline-specific terminology.
   
   - You must make discipline-specific research problems accessible to people outside that discipline. 
   
   - You must focus your discussion and explanations on your research choices.
---

class: inverse, center, middle

# Chapter 8

# Quantitative Methods

---

# Today's Seminar Learning Objectives

## Quantitative methods

1. The three approaches to collecting data: observation, survey, experiment

2. Sampling: How do you do it?

3. Measurement: Operationalisation and variable construction  

4. Analysis: Bivariate hypothesis tests


---

class: segue

# 1. The three approaches to collecting data: observation, survey, experiment


---

### 1. Observational study (no intervention)

* Quantitative description and association of observed variables (e.g, country-level statistics, social media data, documents)

### 2. Survey design (asking questions, the workhorse of social sciences)

* Quantitative description of trends, attitudes, or opinions of a population
* Testing association
* Studying a sample of that population

### 3. Experimental Design (rarer in the social sciences)

* Systematic manipulation of one or more variables to evaluate an outcome
* Holds other variables constant to isolate effects
* Generalize to a broader population

---

### 1. The three approaches to collecting data: observation, survey, experiment

|                       | Observation | Survey | Experiment |
|-----------------------|-------------|--------|------------|
| Intervention          | NO          | YES    | YES        |
| Variable manipulation | NO          | NO     | YES        |


---
class: segue

# 2. Sampling: How do you do it?

---

### Sampling (obsernational study, survey or experiment)

.center[<img src = "../img/lohr-2021-sampling.png" width = '40%'></img>]

* **Target population** The complete collection of observations we want to study.

* **Sample** A subset of a population.

* **Sampled population** The collection of all possible observation units that might have been chosen in a sample.

* **Sampling unit** A unit that can be selected for a sample. Documents, individuals, households.

* **Sampling frame** A list, map, or other specifiation of sampling units in the population from which a sample may be selected. A list of telephone numbers, street names, farms, documents.


.footnote[Lohr, S. L. (2021). Sampling: Design and Analysis. Chapman and Hall/CRC. https://doi.org/10.1201/9780429298899]


---

.center[<img src = "../img/lohr-2021-sampling.png" width = '55%'></img>]

> **Target population** and **sampled population** in a *telephone survey* of *registered voters*. Some persons in the target population do not have a telephone or will not be associated with a telephone number in the sampling frame. In some households with telephones, the residents are not registered to vote and hence are not eligible for the survey. Some eligible persons in the sampling frame population do not respond because they cannot be contacted, some refuse to respond to the survey, and some may be ill and incapable of responding.

.footnote[Lohr, S. L. (2021). Sampling: Design and Analysis. Chapman and Hall/CRC. https://doi.org/10.1201/9780429298899]


---

Once you have defines your **sampling frame** and your **sample size** ($n$=sample size, $N$=population size), you can finally draw your **sample**. 

How do you do it in practice?


### Types of sampling

- **Random sampling**: You randomly (must be true randomness!) select from your sampling frame (i.e., list)

- **Convenience sampling**: You choose your sample based on convenience and access (more practical, but not ideal!)

---

## Population stratification

Your population is stratified based on characteristics of interest before sampling (e.g., gender, ethnicity). This gives more control on the representation of these characteristics in the final sample. 


.center[<img src = 'https://upload.wikimedia.org/wikipedia/commons/f/fa/Stratified_sampling.PNG' width = '35%'></img>]

.content-box-yellow[
Note: The allocation population-sample can be *proportionate* or *disproportionate*, if you want a larger proportion in your sample of some individuals because their characteristics present more variability.
]


---

class: segue-red

# Task 1: Sampling (group)

.pull-right[

.center[<img src = '../img/padlet-week-09-01.png' width = '55%'></img>]

]

---

class: segue

# 3. Measurement: Operationalisation and variable construction  

---

### A quantitative study proves a theory by 

* testing an hypothesis,

* by measuring through operationalisation abstract concepts. 

.center[<img src = "../img/kellstedt-whitten-2013-theory-hypothesis.png", width = "60%"></img>]

.footnote[Kellstedt, P. M., & Whitten, G. D. (2013). The fundamentals of political science research. Cambridge University Press.]
---

### Instrumentation: In your proposal... (from the textbook)
<small>
* Name the survey instrument (e.g. telephone survey) used to collect data
* Indicate how instrument was developed
* Describe the established validity scores (e.g., questions/variables) from past use
  * Content validity
  * Predictive or concurrent validity
  * Construct validity
* Describe reliability of scores from past use
  * Internal consistency
  * Test-retest
* Discuss pilot testing or field-testing
  * Rationale for plans
  * Content validity and reliability
  * Improve question
* Steps for administering for a mailed survey
</small>
---

## Validity vs Reliability (from textbook)

### Validity: can you draw accurate inferences from scores on the instruments?

* **Construct validity**: Does the survey instrument accurately measure the hypothetical construct or concept it was intended to measure?

* **Concurrent or criterion validity**: Does the survey instrument associate with other gold-standard measures of the construct or predict a criterion measure?. 

.content-box-yellow[
Note: **Internal** and **external** validity *mostly* refers to experiment settings. Internal validity is about biases (e.g., selection bias) that can threaten *internal* inferences. External validity is about generalisation of findings outside of the sample, that is *external* inferences about individuals not in the sample.
]

### Reliability: using this instrument (i.e., question) can you consistently repeat the same measurement? 


---

# Probability

---

## Why Probability Matters

Imagine a poll says 23% of Australian voters support a party.

**The poll surveyed:** 1,000 voters (*sample* size, $n$)

**Total Australian voters:** 18.126 million (*population* size, $N$)

  - Note that this is not the whole Australian population (~27.2 million) but Australians on "the electoral roll" according to the Australian Electoral Commission (AEC)

**Question:** How can we trust this represents everyone?

**Answer:** Probability theory!

This is the bridge from *describing data* to *making inferences*.

- Describing the data (i.e. our sample!) is what we call **descriptive statistics**

- Using the sample to make observation about the population is what we call **inferential statistics**

---

## Two Views of What we Should Mean with "Probability"

### Frequentist View

- .content-box-yellow[Probability = long-run frequency]
- "If I flip this coin infinitely many times, 50% will be heads"
- **Objective**, but limited to repeatable events

### Bayesian View

- .content-box-yellow[Probability = degree of belief]
- "I'm 80% confident it will rain tomorrow"
- More flexible, but **subjective**

We'll use the frequentist approach (most common in the social sciences).


---

## Basic Probability Rules
**Key Concepts**
- Probabilities range from 0 (impossible) to 1 (certain)
- All possible outcomes sum to 1
- $P(not A) = 1 - P(A)$

**Example: Rolling a die**
- P(rolling a 4) = 1/6 ‚âà 0.167
- P(not rolling a 4) = 5/6 ‚âà 0.833

---

## Probability Distributions
**What is a Probability Distribution?**
- Describes how probabilities are spread across possible outcomes
- Two types:
  - **Discrete**: Countable outcomes (e.g., coin flips)
  - **Continuous**: Infinite possible values (e.g., height, IQ)

---

## The Binomial Distribution
**For Counting "Successes"**
- Used when:
  - Fixed number of **trials** ( $N$ )
  - Each trial has two **outcomes** (success/failure)
  - Probability of success ( $Œ∏$ ) is **constant** (probability describes the underlying model for generating the data, in this case a dice)
  
.pull-left[

.center[<img src = '../img/skull-dice.png' width = '40%'>]

]

.pull-right[

**Example**: Rolling 20 dice, counting "üíÄ"
- $N$ = 20 trials
- $Œ∏$ = 1/6 (probability of "üíÄ"), **the**ta for "**the**oretical probability"

]  



---

## Binomial Distribution - Visual
**Example**: 20 dice rolls, $P(üíÄ) = 1/6$

Most likely outcome: 3-4 skulls
- Getting exactly 10 skulls would be very surprising!
- The distribution shows what to expect

```{r echo = FALSE, fig.width=12, fig.height=4, out.width="100%"}
library(ggplot2)

# Set parameters
set.seed(123)
n_trials <- 20
prob_skull <- 1/6
n_simulations <- 10000

# Generate data
skulls <- rbinom(n_simulations, size = n_trials, prob = prob_skull)
x_values <- 0:20
theoretical_prob <- dbinom(x_values, size = n_trials, prob = prob_skull)

# Prepare data frames
sim_data <- data.frame(skulls = skulls)
theory_data <- data.frame(x = x_values, probability = theoretical_prob)

# ============================================
# CHART 1: Theoretical Distribution Only
# ============================================

chart1 <- ggplot(theory_data, aes(x = x, y = probability)) +
  geom_col(fill = "red", alpha = 0.7, width = 0.7) +
  geom_point(color = "darkred", size = 3) +
  # Highlight 2-4 skulls (most likely range)
  annotate("rect", xmin = 1.5, xmax = 4.5, 
           ymin = 0, ymax = max(theoretical_prob) * 1.1,
           alpha = 0.2, fill = "green", color = "darkgreen", 
           linetype = "dashed", size = 1) +
  annotate("text", x = 3, y = max(theoretical_prob) * 0.95,
           label = "Most likely:\n2-4 skulls", 
           color = "darkgreen", fontface = "bold", size = 5) +
  geom_vline(xintercept = 10, linetype = "dashed", 
             color = "orange", size = 1.2) +
  annotate("text", x = 10.5, y = max(theoretical_prob) * 0.7,
           label = "10 skulls\n(very rare!)", 
           color = "orange", fontface = "bold", size = 4, hjust = 0) +
  labs(title = "Theoretical Binomial Distribution",
       subtitle = "P(skull) = 1/6, n = 20 dice rolls",
       x = "Number of Skulls",
       y = "Probability") +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        panel.grid.minor = element_blank())

# ============================================
# CHART 2: Observed Data Only
# ============================================

chart2 <- ggplot(sim_data, aes(x = skulls)) +
  geom_histogram(aes(y = after_stat(density)), 
                 binwidth = 1, 
                 fill = "steelblue", 
                 color = "white",
                 alpha = 0.8) +
  annotate("text", x = 11, y = max(theoretical_prob) * 0.9,
           label = sprintf("10,000 samples\nMean: %.2f\nSD: %.2f", 
                          mean(skulls), sd(skulls)),
           color = "steelblue", fontface = "bold", size = 3,
           hjust = 0) +
  labs(title = "Observed Results from Data",
       subtitle = "What we actually see in 10,000 experiments",
       x = "Number of Skulls",
       y = "Probability Density") +
  scale_x_continuous(breaks = seq(0, 20, 2), limits = c(0,20)) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        panel.grid.minor = element_blank())

# ============================================
# CHART 3: Overlay - Theory + Observations
# ============================================

chart3 <- ggplot() +
  # Observed data (histogram)
  geom_histogram(data = sim_data, 
                 aes(x = skulls, y = after_stat(density)), 
                 binwidth = 1, 
                 fill = "steelblue", 
                 color = "white",
                 alpha = 0.6) +
  # Theoretical distribution (line and points)
  geom_line(data = theory_data, 
            aes(x = x, y = probability), 
            color = "red", size = 1.5) +
  geom_point(data = theory_data, 
             aes(x = x, y = probability), 
             color = "red", size = 3) +
  # Highlight most likely region (2-4 skulls)
  annotate("rect", xmin = 1.5, xmax = 4.5, 
           ymin = 0, ymax = max(theoretical_prob) * 1.1,
           alpha = 0.15, fill = "green") +
  annotate("text", x = 3, y = max(theoretical_prob) * 0.95,
           label = "2-4 skulls\nmost likely", 
           color = "darkgreen", fontface = "bold", size = 4) +
  # Mark unlikely outcome
  geom_vline(xintercept = 10, linetype = "dashed", 
             color = "orange", size = 1) +
  annotate("text", x = 10.5, y = max(theoretical_prob) * 0.7,
           label = "Very rare!", 
           color = "orange", fontface = "bold", size = 4, hjust = 0) +
  # Legend
  annotate("text", x = 11, y = max(theoretical_prob) * 0.50,
           label = "Blue bars = Observed\nRed line = Theory",
           color = "black", fontface = "bold", size = 3,
           hjust = 0) +
  labs(title = "Theory vs. Reality: Perfect Match!",
       subtitle = "Simulated data matches theoretical predictions",
       x = "Number of Skulls",
       y = "Probability") +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        panel.grid.minor = element_blank())

# ============================================
# Display all three side by side
# ============================================

library(gridExtra)
grid.arrange(chart1, chart2, chart3, ncol = 3)

```


---

## What is the **theoretical** probability of observing an outcome an what is the **observed** frequency and probability

```{r echo = FALSE}
library(knitr)
library(kableExtra)

# Set parameters
set.seed(123)
n_trials <- 20
prob_skull <- 1/6
n_simulations <- 10000

# Generate simulated data
skulls <- rbinom(n_simulations, size = n_trials, prob = prob_skull)

# Calculate theoretical probabilities
x_values <- 0:20
theoretical_prob <- dbinom(x_values, size = n_trials, prob = prob_skull)

# Count observed frequencies
observed_freq <- table(factor(skulls, levels = 0:20))
observed_prob <- as.numeric(observed_freq) / n_simulations

# Create comprehensive comparison table
comparison_table <- data.frame(
  `Number of Skulls` = x_values,
  `Theoretical Probability` = round(theoretical_prob, 4),
  `Theoretical %` = paste0(round(theoretical_prob * 100, 2), "%"),
  `Observed Frequency` = as.numeric(observed_freq),
  `Observed Probability` = round(observed_prob, 4),
  `Observed %` = paste0(round(observed_prob * 100, 2), "%"),
  `Difference` = round(observed_prob - theoretical_prob, 4)
)

```

```{r echo = FALSE, result = 'asis'}
knitr::kable(comparison_table[1:10,], 
             col.names = c("Skulls", "Th. Prob", "Th. %", 
                          "Obs. Freq", "Obs. Prob", "Obs. %", "Diff"),
             align = 'c',
             digits = 10,
             format = "html")
```

---

```{r echo = FALSE, result = 'asis'}
knitr::kable(comparison_table[10:21,], 
             col.names = c("Skulls", "Th. Prob", "Th. %", 
                          "Obs. Freq", "Obs. Prob", "Obs. %", "Diff"),
             align = 'c',
             digits = 10,
             format = "html")
```

---

### 1950s: When Statistics Confronted Industry Deception

#### The smoking - lung cancer link 
- 1950: Doll & Hill publish first major study
- Tobacco industry: "No proof smoking causes cancer"
- Industry claim: Cancer rates same for smokers/non-smokers
- **Statistical claim**: P(cancer) ‚âà 1/6 regardless of smoking

.pull-left[

```{r echo = FALSE, out.width='100%', fig.width = 6, fig.height = 4}
ggplot() +
  # # Observed data (histogram)
  # geom_histogram(data = sim_data, 
  #                aes(x = skulls, y = after_stat(density)), 
  #                binwidth = 1, 
  #                fill = "steelblue", 
  #                color = "white",
  #                alpha = 0.6) +
  # Theoretical distribution (line and points)
  geom_line(data = theory_data, 
            aes(x = x, y = probability), 
            color = "red", size = 1.5) +
  geom_point(data = theory_data, 
             aes(x = x, y = probability), 
             color = "red", size = 3) +
  # # Highlight most likely region (2-4 skulls)
  # annotate("rect", xmin = 1.5, xmax = 4.5, 
  #          ymin = 0, ymax = max(theoretical_prob) * 1.1,
  #          alpha = 0.15, fill = "green") +
  # annotate("text", x = 3, y = max(theoretical_prob) * 0.95,
  #          label = "2-4 skulls\nmost likely", 
  #          color = "darkgreen", fontface = "bold", size = 4) +
  # # Mark unlikely outcome
  # geom_vline(xintercept = 10, linetype = "dashed", 
  #            color = "orange", size = 1) +
  # annotate("text", x = 10.5, y = max(theoretical_prob) * 0.7,
  #          label = "Very rare!", 
  #          color = "orange", fontface = "bold", size = 4, hjust = 0) +
  # # Legend
  # annotate("text", x = 11, y = max(theoretical_prob) * 0.50,
  #          label = "Blue bars = Observed\nRed line = Theory",
  #          color = "black", fontface = "bold", size = 3,
  #          hjust = 0) +
  labs(x = "Lung cancer cases in a sample of size 20",
       y = "Probability") +
  scale_x_continuous(breaks = seq(0, 20, 2)) +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        panel.grid.minor = element_blank())
```


]

--

.pull-right[

```{r echo=FALSE, message=FALSE, warning=FALSE, out.width='100%', fig.width = 8, fig.height = 5}

library(gridExtra)

# Create static snapshots at key points
create_snapshot <- function(n_studies) {
  subset_data <- data.frame(cases = all_trials$cases[1:n_studies])
  mean_obs <- mean(subset_data$cases)
  
  ggplot() +
    geom_line(data = theory_data, aes(x = x, y = probability),
              color = "red", size = 1.2) +
    geom_point(data = theory_data, aes(x = x, y = probability),
               color = "red", size = 2) +
    geom_histogram(data = subset_data, aes(x = cases, y = after_stat(density)),
                   binwidth = 1, fill = "steelblue", color = "white", alpha = 0.7) +
    geom_vline(xintercept = 20 * 1/6, linetype = "dashed", 
               color = "red", size = 1) +
    geom_vline(xintercept = mean_obs, linetype = "dashed",
               color = "steelblue", size = 1) +
    annotate("text", x = 15, y = 0.24,
             label = sprintf("n=%d\nMean=%.1f", n_studies, mean_obs),
             fontface = "bold", size = 4) +
    labs(title = sprintf("After %d Studies", n_studies),
         x = "Cases per 20", y = "Density") +
    scale_x_continuous(breaks = seq(0, 20, 5), limits = c(0, 20)) +
    ylim(0, 0.26) +
    theme_minimal(base_size = 11) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5))
}

# Generate data once
set.seed(1950)
all_trials <- data.frame(cases = rbinom(50, 20, 4/6))
theory_data <- data.frame(x = 0:20, probability = dbinom(0:20, 20, 1/6))

# Create 4 snapshots
p1 <- create_snapshot(1)
p2 <- create_snapshot(10)
p3 <- create_snapshot(25)
p4 <- create_snapshot(50)

grid.arrange(p1, p2, p3, p4, ncol = 2)

```



]

---

## The Normal Distribution
**The "Bell Curve"**
- Most important distribution in statistics
- Describes many natural phenomena
- Defined by two parameters:
  - **Mean ( $\mu$ ) **: Center of distribution
  - **Standard deviation (SD or $\sigma$ )**: Spread of distribution

**Examples**: **Height**, **Weight**, but crucially also **Measurement Errors**

---

## Normal Distribution

**Properties**
- Symmetric around the mean
- 68.3% of data within 1 SD of mean
- 95.4% of data within 2 SD of mean
- 99.7% of data within 3 SD of mean

```{r echo = FALSE, fig.width = 6, fig.height = 4}
library(ggplot2)

# Parameters for adult height (in cm)
mu <- 170      # Mean height (cm) - roughly average adult
sigma <- 10    # Standard deviation (cm)

# Create data for the normal curve
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 1000)
y <- dnorm(x, mean = mu, sd = sigma)
curve_data <- data.frame(x = x, y = y)

# Create the plot
ggplot(curve_data, aes(x = x, y = y)) +
  # The normal curve
  geom_line(size = 1.5, color = "black") +
  
  # Shade 68% region (within 1 SD)
  geom_area(data = subset(curve_data, x >= mu - sigma & x <= mu + sigma),
            aes(x = x, y = y), fill = "steelblue", alpha = 0.3) +
  
  # Shade 95% region (within 2 SD)
  geom_area(data = subset(curve_data, x >= mu - 2*sigma & x <= mu + 2*sigma),
            aes(x = x, y = y), fill = "lightblue", alpha = 0.2) +
  
  # Shade 99.7% region (within 3 SD)
  geom_area(data = subset(curve_data, x >= mu - 3*sigma & x <= mu + 3*sigma),
            aes(x = x, y = y), fill = "lightblue", alpha = 0.1) +
  
  # Vertical lines for mean and SDs
  geom_vline(xintercept = mu, linetype = "solid", color = "red", size = 1) +
  geom_vline(xintercept = c(mu - sigma, mu + sigma), 
             linetype = "dashed", color = "steelblue", size = 0.8) +
  geom_vline(xintercept = c(mu - 2*sigma, mu + 2*sigma), 
             linetype = "dashed", color = "lightblue", size = 0.8) +
  geom_vline(xintercept = c(mu - 3*sigma, mu + 3*sigma), 
             linetype = "dashed", color = "lightblue", size = 0.8) +
  
  # Labels for standard deviations
  annotate("text", x = mu, y = max(y) * 1.05, 
           label = expression(mu), size = 6, color = "red", fontface = "bold") +
  annotate("text", x = mu - sigma, y = max(y) * 0.5, 
           label = expression(mu - sigma), size = 4, color = "steelblue") +
  annotate("text", x = mu + sigma, y = max(y) * 0.5, 
           label = expression(mu + sigma), size = 4, color = "steelblue") +
  
  # Percentage labels
  annotate("text", x = mu, y = max(y) * 0.25, 
           label = "68.3%", size = 5, fontface = "bold", color = "steelblue") +
  annotate("text", x = mu, y = max(y) * 0.15, 
           label = "95.4%", size = 4.5, fontface = "bold", color = "blue") +
  annotate("text", x = mu, y = max(y) * 0.05, 
           label = "99.7%", size = 4, fontface = "bold", color = "darkblue") +
  
  # Labels
  labs(title = "Normal Distribution: Adult Height",
       subtitle = expression(paste("Mean (", mu, ") = 170 cm, Standard Deviation (", 
                                   sigma, ") = 10 cm")),
       x = "Height (cm)",
       y = "Probability Density") +
  
  # X-axis with meaningful breaks
  scale_x_continuous(breaks = seq(140, 200, 10),
                    labels = seq(140, 200, 10)) +
  
  # Theme
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        panel.grid.minor = element_blank())
```

---

## Why the Normal Distribution Matters
**Three Key Reasons**
1. Many real-world variables are normally distributed
2. Averages of random samples tend toward normal (Central Limit Theorem)
3. Makes statistical inference mathematically tractable

*"The normal distribution is... well, normal!"*


---

# Estimating from Samples

---

## Samples and Populations

**Key Distinction**

.pull-left[
**Population**
- All possible observations
- Often theoretical or impractically large
- True parameters (unknown):
  - Mean: $\mu$
  - Standard deviation: $\sigma$
]

.pull-right[
**Sample**
- The data we actually collect
- Finite and observable
- Sample statistics (calculated):
  - Sample mean: $\bar{X}$
  - Sample SD: $s$
]

**Goal**: Use sample statistics to estimate population parameters

---

## Sampling Matters

.content-box-yellow[

Sampling is the act of drawing individuals from a **population** of interest into your **sample** for **measurement**. 

]

**We Can't Measure Everyone!**
- Too expensive
- Too time-consuming
- Sometimes impossible

**Solution**: Study a sample and make inferences
- Political polls: Sample 1,000 voters from millions
- Drug trials: Sample hundreds from entire population

---

## Simple Random Sampling

.content-box-yellow[

But **how** in practice you draw individuals from the population of interest it matters a great deal. 

- A representative small sample is infinite better that a very large non-representative sample. 

- Becase A non-representative sample won't allow you to infer (i.e. estimate) population characteristics (e.g., Internet polls)

]

.pull-left[

**The Gold Standard**
- Every member of population has equal chance of selection
- No bias in selection process
- Allows valid statistical inference

]

.pull-right[
.small[
**Reality Check**
- In practice, not every individual has the same probability of being selected into the sample.
- Many samples are "convenience samples" (e.g. people are reached through phone or email instead of sending people to hunt them down)
- Because of this we must think and discuss carefully about generalisation and study's limitations 
]
]



---

## The Law of Large Numbers

**Bigger Samples ‚Üí Better Estimates**

.pull-left[
**What Happens as Sample Size Grows:**

- Sample mean $\bar{X}$ gets closer to population mean $\mu$
- Estimates become more accurate
- Uncertainty decreases
- Random errors "cancel out"

**Mathematical Statement:**

$$\text{As } N \to \infty, \text{ then } \bar{X} \to \mu$$

*In plain English: With enough data, the sample mean converges to the true population mean*
]

.pull-right[

> "Even the most stupid of men... is convinced that the more observations have been made, the less danger there is of wandering from one's goal"
‚Äî Jacob Bernoulli, Ars Conjectandi (1713)

Why This Matters: This simple intuition forms the mathematical foundation for all of statistics!

]

---

## Sampling Distributions
**A Crucial Concept**
- If we repeated our experiment many times, we'd get different sample means each time
- The **sampling distribution** shows how these sample means are distributed

**Example**: 
1. Sample 10 people, measure heights
   - Calculate mean ‚Üí $\bar{X}_1 = 168$ cm
2. Sample 10 different people
   - Calculate mean ‚Üí $\bar{X}_2 = 172$ cm
3. Sample 10 more people
   - Calculate mean ‚Üí $\bar{X}_3 = 171$ cm
4. Repeat 10,000 times...
5. Plot all these means ‚Üí **sampling distribution**

---

## Sampling Distribution - Visual

```{r echo = FALSE, fig.width = 10, fig.height = 4, out.width = "80%"}
library(ggplot2)
library(grid)
library(gridExtra)

set.seed(456)
pop_mean <- 170
pop_sd <- 10

# Create three plots with different sample sizes
create_sampling_plot <- function(n, title_text) {
  sample_means <- replicate(10000, mean(rnorm(n, pop_mean, pop_sd)))
  se <- pop_sd / sqrt(n)
  
  ggplot(data.frame(means = sample_means), aes(x = means)) +
    geom_histogram(aes(y = after_stat(density)), 
                   bins = 40, fill = "steelblue", 
                   color = "white", alpha = 0.7) +
    stat_function(fun = dnorm, 
                  args = list(mean = pop_mean, sd = se),
                  color = "red", size = 1.2) +
    geom_vline(xintercept = pop_mean, color = "red", 
               linetype = "dashed", size = 1) +
    labs(title = title_text,
         subtitle = sprintf("SE = %.2f", se),
         x = expression(bar(X)),
         y = "Density") +
    xlim(160, 180) +
    theme_minimal(base_size = 11) +
    theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 12),
          plot.subtitle = element_text(hjust = 0.5, size = 10))
}

p1 <- create_sampling_plot(5, "n = 5")
p2 <- create_sampling_plot(10, "n = 10")
p3 <- create_sampling_plot(30, "n = 30")

grid.arrange(p1, p2, p3, ncol = 3,
             top = textGrob("How Sample Size Affects Sampling Distribution",
                           gp = gpar(fontsize = 16, fontface = "bold")))
```

.pull-left[

.small[

**Distribution of Sample Means**
- Center: Population mean ( $\mu$ = 100 )
- Spread: Standard error ( $SE$ )
- Shape: Approximately normal (especially for large N)

]

]

.pull-right[

.content-box-yellow[

**Key insight**: 

Sample means vary, but cluster around truth!

]

]

---

## The Central Limit Theorem

**The Magic of Averaging**

.pull-left[
**The Theorem:**

No matter what the population distribution looks like (skewed, uniform, bimodal), the distribution of sample means will be approximately normal when $N$ is large enough.

$$\text{As } N \to \infty: \quad \bar{X} \sim N\left(\mu, \frac{\sigma^2}{N}\right)$$
]



.pull-right[

**Three Guarantees:**

1. **Center**: $E(\bar{X}) = \mu$
   - Sample means average to true mean

2. **Spread**: $SE(\bar{X}) = \frac{\sigma}{\sqrt{N}}$
   - Precision increases with sample size

3. **Shape**: $\bar{X}$ is approximately normal
   - Even if original data aren't normal!
]

---

```{r echo=FALSE, fig.width=5, fig.height=6}
library(ggplot2)
library(gridExtra)
# Show CLT with skewed population
set.seed(42)
# Skewed population
pop <- rexp(10000, rate = 0.1)
p1 <- ggplot(data.frame(x = pop), aes(x = x)) +
geom_histogram(bins = 50, fill = "coral", alpha = 0.7) +
labs(title = "Population (Skewed!)",
x = "", y = "Count") +
theme_minimal(base_size = 10)
# Sampling distributions
means_n5 <- replicate(5000, mean(sample(pop, 5)))
means_n30 <- replicate(5000, mean(sample(pop, 30)))
p2 <- ggplot(data.frame(x = means_n5), aes(x = x)) +
geom_histogram(bins = 40, fill = "lightblue", alpha = 0.7) +
labs(title = "Sample Means (n=5)",
x = "", y = "Count") +
theme_minimal(base_size = 10)
p3 <- ggplot(data.frame(x = means_n30), aes(x = x)) +
geom_histogram(bins = 40, fill = "steelblue", alpha = 0.7) +
labs(title = "Sample Means (n=30) - Normal!",
x = expression(bar(X)), y = "Count") +
theme_minimal(base_size = 10)
grid.arrange(p1, p2, p3, ncol = 1)
```

**Key Insight**: This is why we can use normal-based methods even when data aren't normal!

---

# Hypothesis testing

---

## The Logic of Hypothesis Testing

**Case Study: Smoking and Lung Cancer**

### The 1950s Debate

**Tobacco Industry Position:**
> "There is no proof that smoking causes lung cancer. Cancer rates are the same whether you smoke or not."

**Claimed probability**: $P(\text{cancer}) = \frac{1}{6} \approx 17\%$ for everyone (smokers and non-smokers in the adult population)

## This is our baseline hypothesis (a.k.a. the *null hypothesis*). 

.content-box-green[

$H_0$ Smokers have the same probability of getting lung cancer as adult non-smokers. 

]



---

.pull-left[

### The Scientific Evidence

**Doll & Hill (1950) Study:**
- Followed 100 heavy smokers for 20 years
- Result: 13 developed lung cancer

**The Question:**

Is 13 cases significantly different from the 3-4 we'd expect if the industry claim were true?

]

.pull-right[

```{r echo=FALSE, fig.width=6, fig.height=6, out.width='100%'}
library(ggplot2)

# Show expected vs observed
data <- data.frame(
  Group = c("Expected\n(Industry Claim)", "Observed\n(Scientists)"),
  Cases = c(3.3, 13),
  Color = c("Industry", "Reality")
)

ggplot(data, aes(x = Group, y = Cases, fill = Color)) +
  geom_col(width = 0.6, alpha = 0.8) +
  geom_text(aes(label = Cases), vjust = -0.5, 
            fontface = "bold", size = 6) +
  scale_fill_manual(values = c("Industry" = "lightblue", 
                                "Reality" = "darkred")) +
  labs(title = "Lung Cancer Cases per 100 Smokers",
       subtitle = "Expected vs. Observed",
       y = "Number of Cancer Cases",
       x = "") +
  theme_minimal(base_size = 14) +
  theme(plot.title = element_text(face = "bold", hjust = 0.5, size = 16),
        plot.subtitle = element_text(hjust = 0.5, size = 12),
        legend.position = "none") +
  ylim(0, 15)
```

]






The Statistical Question:
Could we see 13 cases by chance alone if the true rate is only 3-4 per 100?
Hypothesis testing provides a formal framework for answering this.

---

## Null and Alternative Hypotheses

**Setting Up the Test**

.pull-left[
**Null Hypothesis (H‚ÇÄ)**

- The "boring" hypothesis
- Usually "no effect" or "no difference"  
- What the establishment claims

**Smoking example**: 
$$H_0: \theta = \frac{1}{6}$$

"Smoking doesn't increase cancer risk. Cancer rate for smokers is same as general population (1/6 or 17%)"

**This is what we test AGAINST**
]

.pull-right[
**Alternative Hypothesis (H‚ÇÅ)**

- What we want to demonstrate
- The "interesting" claim
- Usually involves an effect or difference

**Smoking example**: 
$$H_1: \theta \neq \frac{1}{6}$$

"Smoking DOES affect cancer risk. Cancer rate for smokers is different from 1/6"

**This is what we want to PROVE**
]

.center[**Goal**: Use data to show null hypothesis is right or wrong.]

---

## Hypothesis Testing as a Trial
**Legal Analogy**

- **Defendant**: Null hypothesis
- **Prosecutor**: Researcher
- **Judge**: Statistical test
- **Presumption of innocence**: Assume $H_0$ is true
- **Burden of proof**: Must prove "beyond reasonable doubt"

**We protect the null hypothesis from false conviction!**

.content-box-yellow[

- Yet no single study should completely convince us!
- What we want instead is **replication**: to reject $H_0$ consistently across many independent studies
- A statistical test will never tell us that an outcome is impossible, only that an outcome is unlikely given our baseline assumption (the $H_0$): It's not impossible to roll 20 "6" in a row, just unlikely under the assumption that the dice is fair.

]

---

## The $p$-value

**The Most Misunderstood Concept**

**What it is**:

- Probability of observing data this extreme (or more extreme) if $H_0$ is true
- Small $p$-value = data are surprising under $H_0$

**What it is NOT**:
- ‚úó Probability that $H_0$ is true
- ‚úó Probability that results are due to chance
- ‚úó Importance of the effect

**Decision rule**: If $p < 0.05$, reject $H_0$

---

## Significance Levels

**Conventional Standards in the social sciences**

| **p-value** | **Interpretation** | **Stars** |
|---|---|---|
| p > .05 | Not significant | - |
| p ‚â§ .05 | Significant | * |
| p ‚â§ .01 | Highly significant | ** |
| p ‚â§ .001 | Very highly significant | *** |

---

# Comparing two means

---

## Slide 35: t-tests

**When to use**:

- Outcome variable is continuous (interval/ratio scale)
- Want to compare means between groups
- One of the most common statistical tests with survey results

**Three types**:
1. One-sample t-test
2. Independent samples t-test
3. Paired samples t-test

---

## Slide 36: One-Sample t-test
**Comparing Sample to Known Value**

**Question**: Is our sample mean different from a hypothesised value?

```{r echo=FALSE}
aus_mean <- 
  175
aus_team <- 
  c(206,  190, 204, 210, 194, 209, 207, 212, 211, 201, 198, 203, 191, 206)
this_test <- 
  t.test(aus_team, mu = aus_mean)
```


**Example**: 
- The average height for an adult male in Australia is `r aus_mean` cm.
- Here are the heights in cm of the current roster of Australia men's volleyball team: `r paste(aus_team, collapse = ", ")`. 
- Is this significantly different?

**Test statistic**:

$$t = \frac{\bar{x} - \mu_0}{\bar{\sigma}/\sqrt{n}}$$

---

## Let's do some math!

$$t = \frac{\bar{x} - \mu_0}{\bar{\sigma}/\sqrt{n}}$$

- Adult male population mean ( $\mu_0$ ) or the *true value*: `r aus_mean` cm

- Team's mean ( $\bar{\bar{x}}$ ): `r mean(aus_team)` cm

- Team's standard deviation ( $\bar{\sigma}$ ): `r sd(aus_team)` cm

- Team's size ( $n$ ): `r length(aus_team)`

$$t = \frac{`r mean(aus_team)` - `r aus_mean`}{`r sd(aus_team)`/\sqrt{`r length(aus_team)`}} = \frac{28}{1.944279} = `r this_test$statistic`$$
Nice, but what about the $p$-value? Once you have your test statistics (`r this_test$statistic`), you can get the corresponding p-value by looking into a statistical table or more likely using a statistical software like Jasp. 

In this case, $t = `r this_test$statistic`$, $p$-value = `r this_test$p.value`, which is p < 0.05. Not surprisingly, we can reject the null hypothesis. The difference in averages (sample's mean vs *true value*) is .content-box-yellow[statistically significant].



---

## Independent Samples t-test

**Comparing Two Different Groups**

**Question**: Do two groups have different means? (i.e., a statistically significant difference)

```{r echo = FALSE}
wvs_7 <- read.csv("~/Documents/GitHub/GOVT6139/data/wvs_7.csv")
aus <- wvs_7$Q121[wvs_7$country %in% "AUS"]
uk <- wvs_7$Q121[wvs_7$country %in% "GBR"]
```

Let's use data from the World Value Survey (2022). The WVS collect survey data from different countries. We focus here on this question:

> Q121: How would you evaluate the impact of [immigrants] on the development of [your country]?

Possible answers are ordered (and therefore quantifiable) on a Likert scale

| Very good | Quite good | Neither good, nor bad | Quite bad | Very bad |
|---|---|---||---|---|
| 5 | 4 | 3 | 2 | 1 |

.content-box-yellow[

Do Australians respond differently compared to UK respondents, and is this difference statistically significant?

]

.footnote[Haerpfer, C., Inglehart, R., Moreno, A., Welzel, C., Kizilova, K., Diez-Medrano J., M. Lagos, P. Norris, E. Ponarin & B. Puranen (eds.). 2022. World Values Survey: Round Seven ‚Äì Country-Pooled Datafile Version 6.0. Madrid, Spain & Vienna, Austria: JD Systems Institute & WVSA Secretariat. doi:10.14281/18241.24]

---

## WVS Data and Test

- Australia: M = `r mean(aus)`, SD = `r sd(aus)`, N = `r length(aus)`
- UK: M = `r mean(uk)`, SD = `r sd(uk)`, N = `r length(uk)`

### Welch Two Sample t-test

And here the result from the test!

.center[<img src = '../img/wvs-jasp-ttest.png' width = '45%'>]

### Is the difference statistically significant?


---

class: segue-clue

# Check-in

---

class: segue-red

# Task 2: Hypothesis and questions (individual)

.pull-right[

.center[<img src = '../img/padlet-week-09-02.png' width = '55%'></img>]

]

---